{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd57d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crowdsource_catalogs_long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:279: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:281: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:279: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:281: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing webbpsf\n",
      "Webbpsf version: 2.1.0\n",
      "Done with imports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/20412950/ipykernel_4059184/1456596373.py:61: DeprecationWarning: \"\n",
      "        This message is for information only and WebbPSF will continue to function as normal.\n",
      "        The WebbPSF library has been moved/renamed to STPSF.\n",
      "        Please see https://stpsf.readthedocs.io/en/stable/ for more information.\n",
      "        WebbPSF is now an alias of STPSF and is running code from the STPSF library.\n",
      "        \n",
      "  from webbpsf.utils import to_griddedpsfmodel\n",
      "/scratch/local/20412950/ipykernel_4059184/1456596373.py:279: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  result.meta['visit'] = visitid_[-3:] if visitid_ is not '' else None\n",
      "/scratch/local/20412950/ipykernel_4059184/1456596373.py:281: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  result.meta['vgroup'] = vgroupid_[-4:] if vgroupid_ is not '' else None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matches found to /orange/adamginsburg/jwst/w51//f/pipeline/jw06151001*nrca*_cal.fits",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1268\u001b[39m\n\u001b[32m   1265\u001b[39m             fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[\u001b[32m1\u001b[39m]\n\u001b[32m   1266\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mException \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m was in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc_tb.tb_lineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 647\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(smoothing_scales, bg_boxsizes, filternames, each_exposure, each_suffix, proposal_id, target, modules, crowdsource_default_kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m visitid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, nvisits[proposal_id][target] + \u001b[32m1\u001b[39m):\n\u001b[32m    645\u001b[39m     visitid = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisitid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     filenames = \u001b[43mget_filenames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisitid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisitid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m                              \u001b[49m\u001b[43meach_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43meach_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpupil\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filenames) > \u001b[32m0\u001b[39m:\n\u001b[32m    652\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLooping over filenames \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilenames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for filter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiltername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m proposal=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproposal_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m field=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m visitid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisitid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 696\u001b[39m, in \u001b[36mget_filenames\u001b[39m\u001b[34m(basepath, filtername, proposal_id, field, each_suffix, module, pupil, visitid)\u001b[39m\n\u001b[32m    694\u001b[39m         fglob.remove(st)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fglob) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo matches found to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fglob\n",
      "\u001b[31mValueError\u001b[39m: No matches found to /orange/adamginsburg/jwst/w51//f/pipeline/jw06151001*nrca*_cal.fits"
     ]
    }
   ],
   "source": [
    "print(\"Starting crowdsource_catalogs_long\", flush=True)\n",
    "import glob\n",
    "import time\n",
    "import numpy\n",
    "import crowdsource\n",
    "import regions\n",
    "import numpy as np\n",
    "from functools import cache\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "from astropy.table import Table\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "from astropy import wcs\n",
    "from astropy import table\n",
    "from astropy import stats\n",
    "from astropy import units as u\n",
    "from astropy.nddata import NDData\n",
    "from astropy.io import fits\n",
    "from scipy import ndimage\n",
    "import requests\n",
    "import requests.exceptions\n",
    "import urllib3\n",
    "import urllib3.exceptions\n",
    "from photutils.detection import DAOStarFinder, IRAFStarFinder\n",
    "from photutils.psf import IntegratedGaussianPRF, extract_stars, EPSFStars, EPSFModel\n",
    "try:\n",
    "    # version >=1.7.0, doesn't work: the PSF is broken (https://github.com/astropy/photutils/issues/1580?)\n",
    "    from photutils.psf import PSFPhotometry, IterativePSFPhotometry, SourceGrouper\n",
    "except:\n",
    "    # version 1.6.0, which works\n",
    "    from photutils.psf import BasicPSFPhotometry as PSFPhotometry, IterativelySubtractedPSFPhotometry as IterativePSFPhotometry, DAOGroup as SourceGrouper\n",
    "try:\n",
    "    from photutils.background import MMMBackground, MADStdBackgroundRMS, MedianBackground, Background2D, LocalBackground\n",
    "except:\n",
    "    from photutils.background import MMMBackground, MADStdBackgroundRMS, MedianBackground, Background2D\n",
    "    from photutils.background import MMMBackground as LocalBackground\n",
    "\n",
    "from photutils.psf import EPSFBuilder\n",
    "from photutils.psf import extract_stars\n",
    "\n",
    "import warnings\n",
    "from astropy.utils.exceptions import AstropyWarning, AstropyDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=AstropyWarning)\n",
    "warnings.simplefilter('ignore', category=AstropyDeprecationWarning)\n",
    "\n",
    "from crowdsource import crowdsource_base\n",
    "from crowdsource.crowdsource_base import fit_im, psfmod\n",
    "\n",
    "from astroquery.svo_fps import SvoFps\n",
    "from astropy.table import Table, vstack\n",
    "\n",
    "import pylab as pl\n",
    "pl.rcParams['figure.facecolor'] = 'w'\n",
    "pl.rcParams['image.origin'] = 'lower'\n",
    "\n",
    "import os\n",
    "print(\"Importing webbpsf\", flush=True)\n",
    "import stpsf as webbpsf\n",
    "print(f\"Webbpsf version: {webbpsf.__version__}\")\n",
    "from webbpsf.utils import to_griddedpsfmodel\n",
    "import datetime\n",
    "print(\"Done with imports\", flush=True)\n",
    "import sys\n",
    "\n",
    "def print(*args, **kwargs):\n",
    "    now = datetime.datetime.now().isoformat()\n",
    "    from builtins import print as printfunc\n",
    "    return printfunc(f\"{now}:\", *args, **kwargs)\n",
    "\n",
    "\n",
    "class WrappedPSFModel(crowdsource.psf.SimplePSF):\n",
    "    \"\"\"\n",
    "    wrapper for photutils GriddedPSFModel\n",
    "    \"\"\"\n",
    "    def __init__(self, psfgridmodel, stampsz=19):\n",
    "        self.psfgridmodel = psfgridmodel\n",
    "        self.default_stampsz = stampsz\n",
    "\n",
    "    def __call__(self, col, row, stampsz=None, deriv=False):\n",
    "\n",
    "        if stampsz is None:\n",
    "            stampsz = self.default_stampsz\n",
    "\n",
    "        parshape = numpy.broadcast(col, row).shape\n",
    "        tparshape = parshape if len(parshape) > 0 else (1,)\n",
    "\n",
    "        # numpy uses row, column notation\n",
    "        rows, cols = np.indices((stampsz, stampsz)) - (np.array([stampsz, stampsz])-1)[:, None, None] / 2.\n",
    "\n",
    "        # explicitly broadcast\n",
    "        col = np.atleast_1d(col)\n",
    "        row = np.atleast_1d(row)\n",
    "        #rows = rows[:, :, None] + row[None, None, :]\n",
    "        #cols = cols[:, :, None] + col[None, None, :]\n",
    "\n",
    "        # photutils seems to use column, row notation\n",
    "        # only works with photutils <= 1.6.0 - but is wrong there\n",
    "        #stamps = self.psfgridmodel.evaluate(cols, rows, 1, col, row)\n",
    "        # it returns something in (nstamps, row, col) shape\n",
    "        # pretty sure that ought to be (col, row, nstamps) for crowdsource\n",
    "\n",
    "        # andrew saydjari's version here:\n",
    "        # it returns something in (nstamps, row, col) shape\n",
    "        stamps = []\n",
    "        for i in range(len(col)):\n",
    "            # the +0.5 is required to actually center the PSF (empirically)\n",
    "            #stamps.append(self.psfgridmodel.evaluate(cols+col[i]+0.5, rows+row[i]+0.5, 1, col[i], row[i]))\n",
    "            # the above may have been true when we were using (incorrectly) offset PSFs\n",
    "            stamps.append(self.psfgridmodel.evaluate(cols+col[i], rows+row[i], 1, col[i], row[i]))\n",
    "\n",
    "        stamps = np.array(stamps)\n",
    "\n",
    "        # for oversampled stamps, they may not be normalized\n",
    "        stamps /= stamps.sum(axis=(1,2))[:,None,None]\n",
    "        # this is evidently an incorrect transpose\n",
    "        #stamps = np.transpose(stamps, axes=(0,2,1))\n",
    "\n",
    "        if deriv:\n",
    "            dpsfdrow, dpsfdcol = np.gradient(stamps, axis=(1, 2))\n",
    "\n",
    "        ret = stamps\n",
    "        if parshape != tparshape:\n",
    "            ret = ret.reshape(stampsz, stampsz)\n",
    "            if deriv:\n",
    "                dpsfdrow = dpsfdrow.reshape(stampsz, stampsz)\n",
    "                dpsfdcol = dpsfdcol.reshape(stampsz, stampsz)\n",
    "        if deriv:\n",
    "            ret = (ret, dpsfdcol, dpsfdrow)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def render_model(self, col, row, stampsz=None):\n",
    "        \"\"\"\n",
    "        this function likely does nothing?\n",
    "        \"\"\"\n",
    "        if stampsz is not None:\n",
    "            self.stampsz = stampsz\n",
    "\n",
    "        rows, cols = np.indices(self.stampsz, dtype=float) - (np.array(self.stampsz)-1)[:, None, None] / 2.\n",
    "\n",
    "        return self.psfgridmodel.evaluate(cols, rows, 1, col, row).T.squeeze()\n",
    "\n",
    "\n",
    "def save_epsf(epsf, filename, overwrite=True):\n",
    "    header = {}\n",
    "    header['OVERSAMP'] = list(epsf.oversampling)\n",
    "    hdu = fits.PrimaryHDU(data=epsf.data, header=header)\n",
    "    hdu.writeto(filename, overwrite=overwrite)\n",
    "\n",
    "\n",
    "def read_epsf(filename):\n",
    "    fh = fits.open(filename)\n",
    "    hdu = fh[0]\n",
    "    return EPSFModel(data=hdu.data, oversampling=hdu.header['OVERSAMP'])\n",
    "\n",
    "\n",
    "def catalog_zoom_diagnostic(data, modsky, zoomcut, stars):\n",
    "\n",
    "    # make sure stars is a table\n",
    "    try:\n",
    "        'qf' in stars.colnames\n",
    "    except AttributeError:\n",
    "        stars = Table(stars)\n",
    "\n",
    "    pl.figure(figsize=(12,12))\n",
    "    im = pl.subplot(2,2,1).imshow(data[zoomcut],\n",
    "                                  norm=simple_norm(data[zoomcut],\n",
    "                                                   stretch='log',\n",
    "                                                   max_percent=99.95,\n",
    "                                                   min_cut=0), cmap='gray')\n",
    "    pl.xticks([]); pl.yticks([]); pl.title(\"Data\")\n",
    "    pl.colorbar(mappable=im)\n",
    "    im = pl.subplot(2,2,2).imshow(modsky[zoomcut],\n",
    "                                  norm=simple_norm(modsky[zoomcut],\n",
    "                                                   stretch='log',\n",
    "                                                   max_percent=99.95,\n",
    "                                                   min_cut=0), cmap='gray')\n",
    "    pl.xticks([]); pl.yticks([]); pl.title(\"fit_im model+sky\")\n",
    "    pl.colorbar(mappable=im)\n",
    "\n",
    "    resid = (data[zoomcut] - modsky[zoomcut])\n",
    "    rms = stats.mad_std(resid, ignore_nan=True)\n",
    "    if np.isnan(rms):\n",
    "        raise ValueError(\"RMS is nan, this shouldn't happen\")\n",
    "\n",
    "    norm = (simple_norm(resid, stretch='asinh', max_percent=99.95, min_percent=0.5)\n",
    "            if np.nanmin(resid) > 0 else\n",
    "            simple_norm(resid, stretch='log', max_cut=np.nanpercentile(resid, 99.95), min_cut=-2*rms))\n",
    "\n",
    "    im = pl.subplot(2,2,3).imshow(resid,\n",
    "                                  norm=norm,\n",
    "                                  cmap='gray')\n",
    "    pl.xticks([]); pl.yticks([]); pl.title(f\"data-modsky (rms={rms:10.3g})\")\n",
    "    pl.colorbar(mappable=im)\n",
    "    im = pl.subplot(2,2,4).imshow(data[zoomcut],\n",
    "                                  norm=simple_norm(data[zoomcut],\n",
    "                                                   stretch='log',\n",
    "                                                   max_percent=99.95,\n",
    "                                                   min_cut=0), cmap='gray')\n",
    "\n",
    "    if 'qf' in stars.colnames:\n",
    "        # used in analysis\n",
    "        qgood = ((stars['qf'] > 0.9) &\n",
    "                 (stars['spread_model'] < 0.25) &\n",
    "                 (stars['fracflux'] > 0.75)\n",
    "                )\n",
    "        neg = stars['flux'] < 0\n",
    "    elif 'qfit' in stars.colnames:\n",
    "        # guesses, no tests don\n",
    "        qgood = ((stars['qfit'] < 0.4) &\n",
    "                 (stars['cfit'] < 0.1) &\n",
    "                 (stars['flags'] == 0))\n",
    "        neg = stars['flux_fit'] < 0\n",
    "    else:\n",
    "        qgood = np.ones(len(stars), dtype='bool')\n",
    "        neg = np.zeros(len(stars), dtype='bool')\n",
    "\n",
    "    axlims = pl.axis()\n",
    "    if zoomcut[0].start:\n",
    "        # pl.axis([0,zoomcut[0].stop-zoomcut[0].start, 0, zoomcut[1].stop-zoomcut[1].start])\n",
    "        ok = ((stars['x'] >= zoomcut[1].start) &\n",
    "              (stars['x'] <= zoomcut[1].stop) &\n",
    "              (stars['y'] >= zoomcut[0].start) &\n",
    "              (stars['y'] <= zoomcut[0].stop))\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][ok & ~qgood]-zoomcut[1].start,\n",
    "                                  stars['y'][ok & ~qgood]-zoomcut[0].start,\n",
    "                                  marker='+', color='y', s=8, linewidth=0.5)\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][ok & qgood]-zoomcut[1].start,\n",
    "                                  stars['y'][ok & qgood]-zoomcut[0].start,\n",
    "                                  marker='x', color='r', s=8, linewidth=0.5)\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][neg]-zoomcut[1].start,\n",
    "                                  stars['y'][neg]-zoomcut[0].start,\n",
    "                                  marker='1', color='b', s=8, linewidth=0.5)\n",
    "    else:\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][~qgood],\n",
    "                                  stars['y'][~qgood], marker='+', color='lime', s=5, linewidth=0.5)\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][qgood],\n",
    "                                  stars['y'][qgood], marker='x', color='r', s=5, linewidth=0.5)\n",
    "        pl.subplot(2,2,4).scatter(stars['x'][neg],\n",
    "                                  stars['y'][neg], marker='1', color='b', s=5, linewidth=0.5)\n",
    "    pl.axis(axlims)\n",
    "    pl.xticks([]); pl.yticks([]); pl.title(\"Data with stars\");\n",
    "    pl.colorbar(mappable=im)\n",
    "    pl.tight_layout()\n",
    "\n",
    "\n",
    "def save_photutils_results(result, ww, filename,\n",
    "                           im1, detector,\n",
    "                           basepath, filtername, module, desat, bgsub, exposure_, visitid_, vgroupid_,\n",
    "                           psf=None,\n",
    "                           blur=False,\n",
    "                           basic_or_iterative='basic',\n",
    "                           options=None,\n",
    "                           epsf_=\"\",\n",
    "                           group=\"\",\n",
    "                           fpsf=\"\"):\n",
    "    print(\"Saving photutils results.\")\n",
    "    blur_ = \"_blur\" if blur else \"\"\n",
    "\n",
    "    pixscale = (ww.proj_plane_pixel_area()**0.5).to(u.arcsec)\n",
    "    if 'x_fit' in result.colnames:\n",
    "        coords = ww.pixel_to_world(result['x_fit'], result['y_fit'])\n",
    "        result['skycoord_centroid'] = coords\n",
    "    elif 'xcentroid' in result.colnames:\n",
    "        coords = ww.pixel_to_world(result['xcentroid'], result['ycentroid'])\n",
    "        result['skycoord_centroid'] = coords\n",
    "    elif 'x_init' in result.colnames:\n",
    "        coords = ww.pixel_to_world(result['x_init'], result['y_init'])\n",
    "        result['skycoord_init'] = coords\n",
    "    else:\n",
    "        raise KeyError(f\"No x value found in {result.colnames}\")\n",
    "    print(f'len(result) = {len(result)}, len(coords) = {len(coords)}, type(result)={type(result)}', flush=True)\n",
    "    detector = \"\" # no detector #'s for long\n",
    "    if options.each_exposure:\n",
    "        result.meta['exposure'] = exposure_\n",
    "    if visitid_ is not None:\n",
    "        #result.meta['visit'] = int(visitid_[-3:]) if visitid_ is not '' else None\n",
    "        result.meta['visit'] = visitid_[-3:] if visitid_ is not '' else None\n",
    "    if vgroupid_ is not None:\n",
    "        result.meta['vgroup'] = vgroupid_[-4:] if vgroupid_ is not '' else None\n",
    "        \n",
    "    result.meta['filename'] = filename\n",
    "    result.meta['filter'] = filtername\n",
    "    result.meta['module'] = module\n",
    "    result.meta['detector'] = detector\n",
    "    result.meta['pixscale'] = pixscale.to(u.deg).value\n",
    "    result.meta['pixscale_as'] = pixscale.to(u.arcsec).value\n",
    "    result.meta['proposal_id'] = options.proposal_id\n",
    "\n",
    "    if 'RAOFFSET' in im1[0].header:\n",
    "        result.meta['RAOFFSET'] = im1[0].header['RAOFFSET']\n",
    "        result.meta['DEOFFSET'] = im1[0].header['DEOFFSET']\n",
    "    elif 'RAOFFSET' in im1[1].header:\n",
    "        result.meta['RAOFFSET'] = im1[1].header['RAOFFSET']\n",
    "        result.meta['DEOFFSET'] = im1[1].header['DEOFFSET']\n",
    "\n",
    "    if 'x_err' in result.colnames:\n",
    "        result['dra'] = result['x_err'] * pixscale\n",
    "        result['ddec'] = result['y_err'] * pixscale\n",
    "    #jw06151002001_02101_00001_mirimage_i2d.fits\n",
    "    tblfilename = f\"{basepath}/{filtername}/{filtername.lower()}_{module}{detector}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_{basic_or_iterative}.fits\"\n",
    "\n",
    "    result.write(tblfilename, overwrite=True)\n",
    "\n",
    "    print(\"tblfilename={tblfilename}, filename={filename}, suffix={suffix}, filtername={filtername}, module={module}, desat={desat}, bgsub={bgsub}, fpsf={fpsf} blur={blur}\")\n",
    "\n",
    "    result.write(tblfilename, overwrite=True)\n",
    "    print(f\"Completed {basic_or_iterative} photometry, and wrote out file {tblfilename}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_crowdsource_results(results, ww, filename, suffix,\n",
    "                             im1, detector,\n",
    "                             basepath, filtername, module, desat, bgsub, exposure_, visitid_, vgroupid_,\n",
    "                             psf=None,\n",
    "                             blur=False,\n",
    "                             options=None,\n",
    "                             fpsf=\"\"):\n",
    "    print(\"Saving crowdsource results.\")\n",
    "    blur_ = \"_blur\" if blur else \"\"\n",
    "\n",
    "    stars, modsky, skymsky, psf_ = results\n",
    "    stars = Table(stars)\n",
    "    # crowdsource explicitly inverts x & y from the numpy convention:\n",
    "    # https://github.com/schlafly/crowdsource/issues/11\n",
    "    coords = ww.pixel_to_world(stars['y'], stars['x'])\n",
    "    stars['skycoord'] = coords\n",
    "    stars['x'], stars['y'] = stars['y'], stars['x']\n",
    "    stars['dx'], stars['dy'] = stars['dy'], stars['dx']\n",
    "\n",
    "    pixscale = (ww.proj_plane_pixel_area()**0.5).to(u.arcsec)\n",
    "    stars['dra'] = stars['dx'] * pixscale\n",
    "    stars['ddec'] = stars['dy'] * pixscale\n",
    "\n",
    "    stars.meta['filename'] = filename\n",
    "    stars.meta['filter'] = filtername\n",
    "    stars.meta['module'] = module\n",
    "    stars.meta['detector'] = detector\n",
    "    stars.meta['pixscale'] = pixscale.to(u.deg).value\n",
    "    stars.meta['pixscale_as'] = pixscale.to(u.arcsec).value\n",
    "    stars.meta['proposal_id'] = options.proposal_id\n",
    "    if exposure_:\n",
    "        stars.meta['exposure'] = exposure_\n",
    "    if visitid_:\n",
    "        stars.meta['visit'] = int(visitid_[-3:])\n",
    "    if vgroupid_:\n",
    "        stars.meta['vgroup'] = int(vgroupid_[-4:])\n",
    "\n",
    "    if 'RAOFFSET' in im1[0].header:\n",
    "        stars.meta['RAOFFSET'] = im1[0].header['RAOFFSET']\n",
    "        stars.meta['DEOFFSET'] = im1[0].header['DEOFFSET']\n",
    "    elif 'RAOFFSET' in im1[1].header:\n",
    "        stars.meta['RAOFFSET'] = im1[1].header['RAOFFSET']\n",
    "        stars.meta['DEOFFSET'] = im1[1].header['DEOFFSET']\n",
    "\n",
    "    tblfilename = (f\"{basepath}/{filtername}/\"\n",
    "                   f\"{filtername.lower()}_{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}\"\n",
    "                   f\"_crowdsource_{suffix}.fits\")\n",
    "\n",
    "    print(\"tblfilename={tblfilename}, filename={filename}, suffix={suffix}, filtername={filtername}, module={module}, desat={desat}, bgsub={bgsub}, fpsf={fpsf} blur={blur}\")\n",
    "\n",
    "    stars.write(tblfilename, overwrite=True)\n",
    "    # add WCS-containing header\n",
    "    with fits.open(tblfilename, mode='update', output_verify='fix') as fh:\n",
    "        fh[0].header.update(im1[1].header)\n",
    "    skymskyhdu = fits.PrimaryHDU(data=skymsky, header=im1[1].header)\n",
    "    modskyhdu = fits.ImageHDU(data=modsky, header=im1[1].header)\n",
    "    # PSF doesn't need saving / can't be saved, it's a function\n",
    "    # psfhdu = fits.ImageHDU(data=psf)\n",
    "    hdul = fits.HDUList([skymskyhdu, modskyhdu])\n",
    "    hdul.writeto(f\"{basepath}/{filtername}/{filtername.lower()}_{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}_crowdsource_skymodel_{suffix}.fits\", overwrite=True)\n",
    "\n",
    "    if psf is not None:\n",
    "        if hasattr(psf, 'stamp'):\n",
    "            psfhdu = fits.PrimaryHDU(data=psf.stamp)\n",
    "            psf_fn = (f\"{basepath}/{filtername}/\"\n",
    "                      f\"{filtername.lower()}_{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}\"\n",
    "                      f\"_crowdsource_{suffix}_psf.fits\")\n",
    "            psfhdu.writeto(psf_fn, overwrite=True)\n",
    "        else:\n",
    "            raise ValueError(f\"PSF did not have a stamp attribute.  It was: {psf}, type={type(psf)}\")\n",
    "\n",
    "    return stars\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    fh = fits.open(filename)\n",
    "    im1 = fh\n",
    "    data = im1['SCI'].data\n",
    "    try:\n",
    "        wht = im1['WHT'].data\n",
    "    except KeyError:\n",
    "        wht = None\n",
    "    err = im1['ERR'].data\n",
    "    instrument = im1[0].header['INSTRUME']\n",
    "    telescope = im1[0].header['TELESCOP']\n",
    "    obsdate = im1[0].header['DATE-OBS']\n",
    "    return fh, im1, data, wht, err, instrument, telescope, obsdate\n",
    "\n",
    "\n",
    "def get_psf_model(filtername, proposal_id, field,\n",
    "                  module,\n",
    "                  use_webbpsf=False,\n",
    "                  obsdate=None,\n",
    "                  use_grid=False,\n",
    "                  blur=False,\n",
    "                  target='brick',\n",
    "                  stampsz=19,\n",
    "                  oversample=1,\n",
    "                  basepath='/orange/adamginsburg/jwst/'):\n",
    "    \"\"\"\n",
    "    Return two types of PSF model, the first for DAOPhot and the second for Crowdsource\n",
    "    \"\"\"\n",
    "\n",
    "    basepath = f'{basepath}/{target}'\n",
    "\n",
    "    blur_ = \"_blur\" if blur else \"\"\n",
    "\n",
    "    # psf_fn = f'{basepath}/{instrument.lower()}_{filtername}_samp{oversample}_nspsf{npsf}_npix{fov_pixels}.fits'\n",
    "    # if os.path.exists(str(psf_fn)):\n",
    "    #     # As a file\n",
    "    #     print(f\"Loading grid from psf_fn={psf_fn}\", flush=True)\n",
    "    #     grid = to_griddedpsfmodel(psf_fn)  # file created 2 cells above\n",
    "    #     if isinstance(big_grid, list):\n",
    "    #         print(f\"PSF IS A LIST OF GRIDS!!! this is incompatible with the return from nrc.psf_grid\")\n",
    "    #         grid = grid[0]\n",
    "\n",
    "    if use_webbpsf:\n",
    "        #with open(os.path.expanduser('~/.mast_api_token'), 'r') as fh:\n",
    "        #    api_token = fh.read().strip()\n",
    "        #from astroquery.mast import Mast\n",
    "\n",
    "        #for ii in range(10):\n",
    "        #    try:\n",
    "        #        Mast.login(api_token.strip())\n",
    "        #        break\n",
    "        #    except (requests.exceptions.ReadTimeout, urllib3.exceptions.ReadTimeoutError, TimeoutError) as ex:\n",
    "        #        print(f\"Attempt {ii} to log in to MAST: {ex}\")\n",
    "        #        time.sleep(5)\n",
    "        #os.environ['MAST_API_TOKEN'] = api_token.strip()\n",
    "\n",
    "        has_downloaded = False\n",
    "        ntries = 0\n",
    "        while not has_downloaded:\n",
    "            ntries += 1\n",
    "            try:\n",
    "                print(\"Attempting to download WebbPSF data\", flush=True)\n",
    "                nrc = webbpsf.MIRI()\n",
    "                nrc.load_wss_opd_by_date(f'{obsdate}T00:00:00')\n",
    "                nrc.filter = filtername\n",
    "                if module in ('nrca', 'nrcb'):\n",
    "                    if 'F4' in filtername.upper() or 'F3' in filtername.upper():\n",
    "                        nrc.detector = f'{module.upper()}5' # I think NRCA5 must be the \"long\" detector?\n",
    "                    else:\n",
    "                        nrc.detector = f'{module.upper()}1' #TODO: figure out a way to use all 4?\n",
    "                    # default oversampling is 4\n",
    "                    grid = nrc.psf_grid(num_psfs=16, all_detectors=False, verbose=True, save=True)\n",
    "                elif 'mirimage' in module:\n",
    "                    print('module', module, flush=True)\n",
    "                    print(nrc.detector)\n",
    "                    nrc.detector = 'MIRIM'\n",
    "                    grid = nrc.psf_grid(num_psfs=16, all_detectors=False, verbose=True, save=True)\n",
    "                else:\n",
    "                    grid = nrc.psf_grid(num_psfs=16, all_detectors=True, verbose=True, save=True)\n",
    "                has_downloaded = True\n",
    "            except (urllib3.exceptions.ReadTimeoutError, requests.exceptions.ReadTimeout, requests.HTTPError) as ex:\n",
    "                print(f\"Failed to build PSF: {ex}\", flush=True)\n",
    "            except Exception as ex:\n",
    "                print(ex, flush=True)\n",
    "                if ntries > 10:\n",
    "                    # avoid infinite loops\n",
    "                    raise ValueError(\"Failed to download PSF, probably because of an error listed above\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        if use_grid:\n",
    "            return grid, WrappedPSFModel(grid, stampsz=stampsz)\n",
    "        else:\n",
    "            # there's no way to use a grid across all detectors.\n",
    "            # the right way would be to use this as a grid of grids, but that apparently isn't supported.\n",
    "            if isinstance(grid, list):\n",
    "                grid = grid[0]\n",
    "\n",
    "            #yy, xx = np.indices([31,31], dtype=float)\n",
    "            #grid.x_0 = grid.y_0 = 15.5\n",
    "            #psf_model = crowdsource.psf.SimplePSF(stamp=grid(xx,yy))\n",
    "\n",
    "            # bigger PSF probably needed\n",
    "            yy, xx = np.indices([61, 61], dtype=float)\n",
    "            grid.x_0 = grid.y_0 = 30\n",
    "            psf_model = crowdsource.psf.SimplePSF(stamp=grid(xx, yy))\n",
    "\n",
    "            return grid, psf_model\n",
    "    else:\n",
    "\n",
    "        grid = psfgrid = to_griddedpsfmodel(f'{basepath}/psfs/{filtername.upper()}_{proposal_id}_{field}_merged_PSFgrid_oversample{oversample}{blur_}.fits')\n",
    "\n",
    "        # if isinstance(grid, list):\n",
    "        #     print(f\"Grid is a list: {grid}\")\n",
    "        #     psf_model = WrappedPSFModel(grid[0])\n",
    "        #     dao_psf_model = grid[0]\n",
    "        # else:\n",
    "\n",
    "        psf_model = WrappedPSFModel(grid, stampsz=stampsz)\n",
    "        dao_psf_model = grid\n",
    "\n",
    "        return grid, psf_model\n",
    "\n",
    "\n",
    "def get_uncertainty(err, data, dq=None, wht=None):\n",
    "\n",
    "    if dq is None:\n",
    "        dq = np.zeros(data.shape, dtype='int')\n",
    "\n",
    "    # crowdsource uses inverse-sigma, not inverse-variance\n",
    "    weight = err**-1\n",
    "    #maxweight = np.percentile(weight[np.isfinite(weight)], 95)\n",
    "    #minweight = np.percentile(weight[np.isfinite(weight)], 5)\n",
    "    #badweight =  np.percentile(weight[np.isfinite(weight)], 1)\n",
    "    #weight[err < 1e-5] = 0\n",
    "    #weight[(err == 0) | (wht == 0)] = np.nanmedian(weight)\n",
    "    #weight[np.isnan(weight)] = 0\n",
    "    bad = np.isnan(weight) | (data == 0) | np.isnan(data) | (weight == 0) | (err == 0)\n",
    "    #if dq is not None:\n",
    "    #    # only 0 is OK\n",
    "    #    bad |= (dq != 0)\n",
    "    if wht is not None:\n",
    "        bad |= (wht == 0)\n",
    "\n",
    "    #weight[weight > maxweight] = maxweight\n",
    "    #weight[weight < minweight] = minweight\n",
    "    # it seems that crowdsource doesn't like zero weights\n",
    "    # may have caused broked f466n? weight[bad] = badweight\n",
    "    #weight[bad] = minweight\n",
    "    # crowdsource explicitly handles weight=0, so this _should_ work.\n",
    "    weight[bad] = 0\n",
    "\n",
    "    # Expand bad pixel zones for dq\n",
    "    #bad_for_dq = ndimage.binary_dilation(bad, iterations=2)\n",
    "    #dq[bad_for_dq] = 2 | 2**30 | 2**31\n",
    "    #print(f\"Total bad pixels = {bad.sum()}, total bad for dq={bad_for_dq.sum()}\")\n",
    "\n",
    "    return dq, weight, bad\n",
    "\n",
    "\n",
    "def main(smoothing_scales={'f182m': 0.25, 'f187n':0.25, 'f212n':0.55,\n",
    "                           'f410m': 0.55, 'f405n':0.55, 'f466n':0.55},\n",
    "        bg_boxsizes={'f182m': 19, 'f187n':11, 'f212n':11,\n",
    "                     'f410m': 11, 'f405n':11, 'f466n':11,\n",
    "                     'f444w': 11, 'f356w':11,\n",
    "                     'f200w':19, 'f115w':19,},\n",
    "        filternames='f480m',\n",
    "        each_exposure=True,\n",
    "        each_suffix='cal',\n",
    "        proposal_id='6151',\n",
    "        target='w51',\n",
    "        modules=['nrca', 'nrcb'],\n",
    "                    \n",
    "        crowdsource_default_kwargs={'maxstars': 500000, },\n",
    "        ):\n",
    "    \"\"\"\n",
    "    from optparse import OptionParser\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"-f\", \"--filternames\", dest=\"filternames\",\n",
    "                    default='F140M',\n",
    "                    help=\"filter name list\", metavar=\"filternames\")\n",
    "    parser.add_option(\"-m\", \"--modules\", dest=\"modules\",\n",
    "                    #default='nrca,nrcb,merged,merged-reproject',\n",
    "                    default='nrca,nrcb,merged',\n",
    "                    help=\"module list\", metavar=\"modules\") \n",
    "    parser.add_option(\"-d\", \"--desaturated\", dest=\"desaturated\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"use image with saturated stars removed?\", metavar=\"desaturated\")\n",
    "    parser.add_option(\"--daophot\", dest=\"daophot\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"run daophot?\", metavar=\"daophot\")\n",
    "    parser.add_option(\"--skip-crowdsource\", dest=\"nocrowdsource\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"skip crowdsource?\", metavar=\"nocrowdsource\")\n",
    "    parser.add_option(\"--bgsub\", dest=\"bgsub\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"perform global background-subtraction first?\", metavar=\"bgsub\")\n",
    "    parser.add_option(\"--epsf\", dest=\"epsf\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"try to make & use an ePSF?\", metavar=\"epsf\")\n",
    "    parser.add_option(\"--blur\", dest=\"blur\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"blur the PSF?\", metavar=\"blur\")\n",
    "    parser.add_option(\"--proposal_id\", dest=\"proposal_id\",\n",
    "                    default='6151',\n",
    "                    help=\"proposal_id\", metavar=\"proposal_id\")\n",
    "    parser.add_option(\"--target\", dest=\"target\",\n",
    "                    default='w51',\n",
    "                    help=\"target\", metavar=\"target\")\n",
    "    parser.add_option(\"--group\", dest=\"group\",\n",
    "                      default=False,\n",
    "                      action='store_true')\n",
    "    parser.add_option('--each-exposure', dest='each_exposure',\n",
    "                      default=False, action='store_true',\n",
    "                      help='Photometer _each_ exposure?', metavar='each_exposure')\n",
    "    parser.add_option('--each-suffix', dest='each_suffix',\n",
    "                      #default='destreak_o001_crf',\n",
    "                      default='cal',\n",
    "                      help='Suffix for the level-2 products', metavar='each_suffix')\n",
    "    (options, args) = parser.parse_args()\n",
    "\n",
    "    filternames = options.filternames.split(\",\")\n",
    "    modules = options.modules.split(\",\")\n",
    "    proposal_id = options.proposal_id\n",
    "    target = options.target\n",
    "    \"\"\"\n",
    "\n",
    "    nvisits = {'2221': {'brick': 1, 'cloudc': 2},\n",
    "               '1182': {'brick': 2},\n",
    "               '6151': {'w51': 1, 'w51_miri': 2}\n",
    "               }\n",
    "    field_to_reg_mapping = {'2221': {'001': 'brick', '002': 'cloudc'},\n",
    "                            '1182': {'004': 'brick'},\n",
    "                            '6151': {'001': 'w51', '002':'w51_miri'}}[proposal_id]\n",
    "    reg_to_field_mapping = {v:k for k,v in field_to_reg_mapping.items()}\n",
    "    field = reg_to_field_mapping[target]\n",
    "\n",
    "    basepath = f'/orange/adamginsburg/jwst/w51/'\n",
    "\n",
    "    pl.close('all')\n",
    "\n",
    "    #print(f\"options: {options}\")\n",
    "\n",
    "    # need to have incrementing _before_ test\n",
    "    index = -1\n",
    "\n",
    "    for module in modules:\n",
    "        detector = module # no sub-detectors for long-NIRCAM\n",
    "        for filtername in filternames:\n",
    "            if each_exposure:\n",
    "                for visitid in range(1, nvisits[proposal_id][target] + 1):\n",
    "                    visitid = f'{visitid}'\n",
    "                    \n",
    "                    filenames = get_filenames(basepath, filtername, proposal_id,\n",
    "                                              field, visitid=visitid,\n",
    "                                              each_suffix=each_suffix,\n",
    "                                              module=module, pupil='clear')\n",
    "                    if len(filenames) > 0:\n",
    "                        print(f\"Looping over filenames {filenames} for filter={filtername} proposal={proposal_id} field={field} visitid={visitid}\")\n",
    "                        # jw02221001001_07101_00024_nrcblong_destreak_o001_crf.fits\n",
    "                        for filename in filenames:\n",
    "\n",
    "                            index += 1\n",
    "                            # enable array jobs\n",
    "                            if os.getenv('SLURM_ARRAY_TASK_ID') is not None and int(os.getenv('SLURM_ARRAY_TASK_ID')) != index:\n",
    "                                print(f'Task={os.getenv(\"SLURM_ARRAY_TASK_ID\")} does not match index {index}')\n",
    "                                continue\n",
    "\n",
    "                            exposure_id = filename.split(\"_\")[2]\n",
    "                            visit_id = filename.split(\"_\")[0][-3:]\n",
    "                            vgroup_id = filename.split(\"_\")[1]\n",
    "                            do_photometry_step(options, filtername, module, detector,\n",
    "                                               field, basepath, filename, proposal_id,\n",
    "                                               crowdsource_default_kwargs, exposurenumber=int(exposure_id),\n",
    "                                               visit_id=visit_id, vgroup_id=vgroup_id,\n",
    "                                               use_webbpsf=True,\n",
    "                                               bg_boxsizes=bg_boxsizes)\n",
    "            else:\n",
    "                filename = get_filename(basepath, filtername, proposal_id, field, module, options=options, pupil='clear')\n",
    "                do_photometry_step(options, filtername, module, detector, field,\n",
    "                                   basepath, filename, proposal_id, crowdsource_default_kwargs,\n",
    "                                   bg_boxsizes=bg_boxsizes\n",
    "                                   )\n",
    "\n",
    "\n",
    "def get_filenames(basepath, filtername, proposal_id, field, each_suffix, module, pupil='clear', visitid='001'):\n",
    "\n",
    "    # jw01182004002_02101_00012_nrcalong_destreak_o004_crf.fits\n",
    "    # jw02221001001_07101_00012_nrcalong_destreak_o001_crf.fits\n",
    "    # jw02221001001_05101_00022_nrcb3_destreak_o001_crf.fits\n",
    "        #jw06151002001_02101_00001_mirimage_i2d.fits\n",
    "\n",
    "    glstr = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}{field}*{module}*_{each_suffix}.fits'\n",
    "    \n",
    "  \n",
    "    fglob = glob.glob(glstr)\n",
    "    for st in fglob:\n",
    "        print(st)\n",
    "        if 'align' in st or 'uncal' in st:\n",
    "            print(f\"Removing {st} from glob string because it is an alignment file\")\n",
    "            fglob.remove(st)\n",
    "    if len(fglob) == 0:\n",
    "        raise ValueError(f\"No matches found to {glstr}\")\n",
    "    else:\n",
    "        return fglob\n",
    "\n",
    "\n",
    "def get_filename(basepath, filtername, proposal_id, field, module, options, pupil='clear'):\n",
    "    desat = '_unsatstar' if options.desaturated else ''\n",
    "    bgsub = '_bgsub' if options.bgsub else ''\n",
    "    #epsf_ = \"_epsf\" if options.epsf else \"\"\n",
    "    #blur_ = \"_blur\" if options.blur else \"\"\n",
    "    filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_i2d.fits'\n",
    "\n",
    "\n",
    "    #filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_i2d.fits'\n",
    "    print('finding ', filename)\n",
    "    print(os.path.exists(filename))\n",
    "    if not os.path.exists(filename):\n",
    "        filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_realigned-to-refcat.fits'\n",
    "    if not os.path.exists(filename):\n",
    "        # merged-reproject_i2d.fits lives here\n",
    "        # 12/22/2023: that is generally the best-behaved; it's the only one with no clear misalignments.  tweakreg-based merge just doesn't lock in coords\n",
    "        filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_i2d{desat}.fits'\n",
    "    if not os.path.exists(filename):\n",
    "        pupil = 'F444W'\n",
    "        filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_nodestreak_realigned-to-refcat.fits'\n",
    "    if not os.path.exists(filename):\n",
    "        filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_realigned-to-refcat.fits'\n",
    "    if not os.path.exists(filename):\n",
    "        # merged-reproject_i2d.fits lives here\n",
    "        # 12/22/2023: that is generally the best-behaved; it's the only one with no clear misalignments.  tweakreg-based merge just doesn't lock in coords\n",
    "        filename = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}_i2d{desat}.fits'\n",
    "    if not os.path.exists(filename):\n",
    "        glstr = f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_*-{module}_realigned-to-refcat.fits'\n",
    "        fglob = glob.glob(glstr)\n",
    "        if len(fglob) == 1:\n",
    "            filename = fglob[0]\n",
    "        else:\n",
    "            raise ValueError(f\"File {filename} does not exist, and nothing matching {glstr} exists either.  pupil={pupil}\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def do_photometry_step(options, filtername, module, detector, field, basepath,\n",
    "                       filename, proposal_id, crowdsource_default_kwargs, exposurenumber=None,\n",
    "                       visit_id=None, vgroup_id=None,\n",
    "                       bg_boxsizes=None,\n",
    "                       use_webbpsf=False,\n",
    "                       nsigma=5,\n",
    "                       pupil='clear'):\n",
    "    \"\"\"\n",
    "    nsigma is the threshold to multiply the error estimate by to get the detection threshold\n",
    "    \"\"\"\n",
    "    print(f\"Starting {field} filter {filtername} module {module} detector {detector} {exposurenumber}\", flush=True)\n",
    "    fwhm_tbl = Table.read(f'{basepath}/reduction/fwhm_table.ecsv')\n",
    "    row = fwhm_tbl[fwhm_tbl['Filter'] == filtername]\n",
    "    fwhm = fwhm_arcsec = float(row['PSF FWHM (arcsec)'][0])\n",
    "    fwhm_pix = float(row['PSF FWHM (pixel)'][0])\n",
    "\n",
    "    # redundant, saves me renaming variables....\n",
    "    filt = filtername\n",
    "    print('visit_id', visit_id)\n",
    "\n",
    "    # file naming suffixes\n",
    "    desat = '_unsatstar' if options.desaturated else ''\n",
    "    bgsub = '_bgsub' if options.bgsub else ''\n",
    "    epsf_ = \"_epsf\" if options.epsf else \"\"\n",
    "    exposure_ = f'_exp{exposurenumber:05d}' if exposurenumber is not None else ''\n",
    "    #visitid_ = f'_visit{int(visit_id):03d}' if visit_id is not None else ''\n",
    "    visitid_ = f'_visit{visit_id}' if visit_id is not None else ''\n",
    "    vgroupid_ = f'_vgroup{vgroup_id}' if vgroup_id is not None else ''\n",
    "    blur_ = \"_blur\" if options.blur else \"\"\n",
    "    group = \"_group\" if options.group else \"\"\n",
    "\n",
    "    print(f\"Starting cataloging on {filename}\", flush=True)\n",
    "    fh, im1, data, wht, err, instrument, telescope, obsdate = load_data(filename)\n",
    "\n",
    "    # set up coordinate system\n",
    "    ww = wcs.WCS(im1[1].header)\n",
    "    pixscale = ww.proj_plane_pixel_area()**0.5\n",
    "    cen = ww.pixel_to_world(im1[1].shape[1]/2, im1[1].shape[0]/2)\n",
    "\n",
    "    if options.bgsub:\n",
    "        # background subtraction\n",
    "        # see BackgroundEstimationExperiments.ipynb\n",
    "        bkg = Background2D(data, box_size=bg_boxsizes[filt.lower()], bkg_estimator=MedianBackground())\n",
    "        fits.PrimaryHDU(data=bkg.background,\n",
    "                        header=im1['SCI'].header).writeto(filename.replace(\".fits\",\n",
    "                                                                           \"_background.fits\"),\n",
    "                                                          overwrite=True)\n",
    "\n",
    "        # subtract background, but then re-zero the edges\n",
    "        zeros = data == 0\n",
    "        data = data - bkg.background\n",
    "        data[zeros] = 0\n",
    "\n",
    "        fits.PrimaryHDU(data=data, header=im1['SCI'].header).writeto(filename.replace(\".fits\", \"_bgsub.fits\"), overwrite=True)\n",
    "\n",
    "    # try to limit memory use before we start photometry\n",
    "    data = data.astype('float32')\n",
    "\n",
    "    # Load PSF model\n",
    "    grid, psf_model = get_psf_model(filtername, proposal_id, field,\n",
    "                                    module=module,\n",
    "                                    use_webbpsf=use_webbpsf,\n",
    "                                    # if we're doing each exposure, we want the full grid\n",
    "                                    use_grid=options.each_exposure,\n",
    "                                    blur=options.blur,\n",
    "                                    target=options.target,\n",
    "                                    obsdate=obsdate,\n",
    "                                    basepath='/orange/adamginsburg/jwst/')\n",
    "    dao_psf_model = grid\n",
    "\n",
    "    # bound the flux to be >= 0 (no negative peak fitting)\n",
    "    #dao_psf_model.flux.min = 0\n",
    "\n",
    "    dq, weight, bad = get_uncertainty(err, data, wht=wht, dq=im1['DQ'].data if 'DQ' in im1 else None)\n",
    "\n",
    "    filter_table = SvoFps.get_filter_list(facility=telescope, instrument=instrument)\n",
    "    filter_table.add_index('filterID')\n",
    "    instrument = 'MIRI'\n",
    "    eff_wavelength = filter_table.loc[f'{telescope}/{instrument}.{filt}']['WavelengthEff'] * u.AA\n",
    "\n",
    "    # DAO Photometry setup\n",
    "    grouper = SourceGrouper(2 * fwhm_pix)\n",
    "    mmm_bkg = MMMBackground()\n",
    "\n",
    "    filtered_errest = stats.sigma_clipped_stats(data, stdfunc='mad_std')\n",
    "    print(f'Error estimate for DAO from stats.: {filtered_errest}', flush=True)\n",
    "    filtered_errest = np.nanmedian(err)\n",
    "    print(f'Error estimate for DAO from median(err): {filtered_errest}', flush=True)\n",
    "\n",
    "    daofind_tuned = DAOStarFinder(threshold=nsigma * filtered_errest,\n",
    "                                  fwhm=fwhm_pix, roundhi=1.0, roundlo=-1.0,\n",
    "                                  sharplo=0.30, sharphi=1.40)\n",
    "    #daofind_tuned = DAOStarFinder(threshold=4 * filtered_errest,\n",
    "    #                              fwhm=fwhm_pix, roundhi=0.8, roundlo=-0.9,\n",
    "    #                              sharplo=0.25, sharphi=1.20)\n",
    "    print(\"Finding stars with daofind_tuned\", flush=True)\n",
    "\n",
    "    # empirically determined in debugging session with Taehwa on 2025-12-09:\n",
    "    # with just nan_to_num, setting pixels to zero, some stars got \"erased\"\n",
    "    kernel = Gaussian2DKernel(x_stddev=fwhm_pix/2.355)\n",
    "    mask = np.isnan(data)\n",
    "    if 'DQ' in im1:\n",
    "        dqarr = im1['DQ'].data\n",
    "        is_saturated = (dqarr & dqflags.pixel['SATURATED']) != 0\n",
    "        # we want original data_ to be untouched for imshowing diagnostics etc.\n",
    "        data_ = data.copy()\n",
    "        data_[is_saturated] = np.nan\n",
    "        mask |= is_saturated\n",
    "    else:\n",
    "        data_ = data\n",
    "\n",
    "    nan_replaced_data = interpolate_replace_nans(data_, kernel, convolve=convolve_fft)\n",
    "\n",
    "    finstars = daofind_tuned(nan_replaced_data,\n",
    "                             mask=mask)\n",
    "\n",
    "    print(f\"Found {len(finstars)} with daofind_tuned\", flush=True)\n",
    "    # for diagnostic plotting convenience\n",
    "    finstars['x'] = finstars['xcentroid']\n",
    "    finstars['y'] = finstars['ycentroid']\n",
    "    finstars['skycoord'] = ww.pixel_to_world(finstars['x'], finstars['y'])\n",
    "    print('visitid_', visitid_)\n",
    "    result = save_photutils_results(finstars, ww, filename,\n",
    "                                    im1=im1, detector=detector,\n",
    "                                    basepath=basepath,\n",
    "                                    filtername=filtername, module=module,\n",
    "                                    desat=desat, bgsub=bgsub,\n",
    "                                    blur=\"\",\n",
    "                                    exposure_=exposure_,\n",
    "                                    visitid_=visitid_, vgroupid_=vgroupid_,\n",
    "                                    basic_or_iterative='daofind',\n",
    "                                    options=options,\n",
    "                                    epsf_=\"\",\n",
    "                                    fpsf=\"\",\n",
    "                                    group=group,\n",
    "                                    psf=None)\n",
    "\n",
    "    stars = finstars # because I'm copy-pasting code...\n",
    "\n",
    "    # Set up visualization\n",
    "    reg = regions.RectangleSkyRegion(center=cen, width=1.5*u.arcmin, height=1.5*u.arcmin)\n",
    "    preg = reg.to_pixel(ww)\n",
    "    #mask = preg.to_mask()\n",
    "    #cutout = mask.cutout(im1[1].data)\n",
    "    #err = mask.cutout(im1[2].data)\n",
    "    region_list = [y for x in glob.glob('regions_/*zoom*.reg') for y in\n",
    "                   regions.Regions.read(x)]\n",
    "    zoomcut_list = {reg.meta['text']: reg.to_pixel(ww).to_mask().get_overlap_slices(data.shape)[0]\n",
    "                    for reg in region_list}\n",
    "    zoomcut_list = {nm:slc for nm,slc in zoomcut_list.items()\n",
    "                    if slc is not None and\n",
    "                    slc[0].start > 0 and slc[1].start > 0\n",
    "                    and slc[0].stop < data.shape[0] and slc[1].stop < data.shape[1]}\n",
    "\n",
    "    zoomcut = slice(128, 256), slice(128, 256)\n",
    "    modsky = data*0 # no model for daofind\n",
    "    nullslice = (slice(None), slice(None))\n",
    "\n",
    "    try:\n",
    "        catalog_zoom_diagnostic(data, modsky, nullslice, stars)\n",
    "        pl.suptitle(f\"daofind Catalog Diagnostics zoomed {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}\")\n",
    "        pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}_catalog_diagnostics_daofind.png',\n",
    "                bbox_inches='tight')\n",
    "\n",
    "        catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "        pl.suptitle(f\"daofind Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}\")\n",
    "        pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}_catalog_diagnostics_zoom_daofind.png',\n",
    "                bbox_inches='tight')\n",
    "\n",
    "        for name, zoomcut in zoomcut_list.items():\n",
    "            catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "            pl.suptitle(f\"daofind Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub} zoom {name}\")\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}_catalog_diagnostics_zoom{name.replace(\" \",\"_\")}_daofind.png',\n",
    "                    bbox_inches='tight')\n",
    "    except Exception as ex:\n",
    "        print(f'FAILURE to produce catalog zoom diagnostics for module {module} and filter {filtername} for basic daofinder: {ex}')\n",
    "\n",
    "    if not options.nocrowdsource:\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        if False: # why do the unweighted version?\n",
    "            print()\n",
    "            print(\"starting crowdsource unweighted\", flush=True)\n",
    "            results_unweighted = fit_im(nan_replaced_data, psf_model,\n",
    "                                        weight=np.ones_like(data)*np.nanmedian(weight)*(~mask),\n",
    "                                        # psfderiv=np.gradient(-psf_initial[0].data),\n",
    "                                        dq=dq,\n",
    "                                        nskyx=0, nskyy=0, refit_psf=False, verbose=True,\n",
    "                                        **crowdsource_default_kwargs,\n",
    "                                        )\n",
    "            print(f\"Done with unweighted crowdsource. dt={time.time() - t0}\")\n",
    "            stars, modsky, skymsky, psf = results_unweighted\n",
    "            stars = save_crowdsource_results(results_unweighted, ww, filename,\n",
    "                                             im1=im1, detector=detector,\n",
    "                                             basepath=basepath,\n",
    "                                             filtername=filtername, module=module,\n",
    "                                             desat=desat, bgsub=bgsub,\n",
    "                                             blur=options.blur,\n",
    "                                             exposure_=exposure_,\n",
    "                                             visitid_=visitid_,\n",
    "                                             vgroupid_=vgroupid_,\n",
    "                                             options=options,\n",
    "                                             suffix=\"unweighted\", psf=None)\n",
    "\n",
    "            zoomcut = slice(128, 256), slice(128, 256)\n",
    "\n",
    "            try:\n",
    "                catalog_zoom_diagnostic(data, modsky, nullslice, stars)\n",
    "                pl.suptitle(f\"Crowdsource nsky=0 unweighted Catalog Diagnostics zoomed {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_}\")\n",
    "                pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_}_catalog_diagnostics_unweighted.png',\n",
    "                        bbox_inches='tight')\n",
    "\n",
    "                catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                pl.suptitle(f\"Crowdsource nsky=0 unweighted Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_}\")\n",
    "                pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_}_catalog_diagnostics_zoom_unweighted.png',\n",
    "                        bbox_inches='tight')\n",
    "                for name, zoomcut in zoomcut_list.items():\n",
    "                    catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                    pl.suptitle(f\"Crowdsource nsky=0 Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_} zoom {name}\")\n",
    "                    pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{blur_}_catalog_diagnostics_zoom{name.replace(\" \",\"_\")}_unweighted.png',\n",
    "                            bbox_inches='tight')\n",
    "            except Exception as ex:\n",
    "                print(f'FAILURE to produce catalog zoom diagnostics for module {module} and filter {filtername} for unweighted crowdsource: {ex}')\n",
    "                exc_tb = sys.exc_info()[2]\n",
    "                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                print(f\"Exception {ex} was in {fname} line {exc_tb.tb_lineno}\")\n",
    "\n",
    "            fig = pl.figure(0, figsize=(10,10))\n",
    "            fig.clf()\n",
    "            ax = fig.gca()\n",
    "            im = ax.imshow(weight, norm=simple_norm(weight, stretch='log'))\n",
    "            pl.colorbar(mappable=im)\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}_weights.png',\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "        #for refit_psf, fpsf in zip((False, True), ('', '_fitpsf',)):\n",
    "        for refit_psf, fpsf in zip((False, ), ('', )):\n",
    "            for nsky in (0, ): #1, ):\n",
    "                t0 = time.time()\n",
    "                print()\n",
    "                print(f\"Running crowdsource fit_im with weights & nskyx=nskyy={nsky} & fpsf={fpsf} & blur={blur_}\")\n",
    "                print(f\"data.shape={data.shape} weight_shape={weight.shape}\", flush=True)\n",
    "                results = fit_im(nan_replaced_data, psf_model, weight=weight * (~mask),\n",
    "                                 nskyx=nsky, nskyy=nsky, refit_psf=refit_psf, verbose=True,\n",
    "                                 dq=dq,\n",
    "                                 **crowdsource_default_kwargs\n",
    "                                 )\n",
    "                print(f\"Done with weighted, refit={fpsf}, nsky={nsky} crowdsource. dt={time.time() - t0}\")\n",
    "                stars, modsky, skymsky, psf = results\n",
    "                stars = save_crowdsource_results(results, ww, filename,\n",
    "                                                 im1=im1, detector=detector,\n",
    "                                                 basepath=basepath,\n",
    "                                                 filtername=filtername,\n",
    "                                                 module=module, desat=desat,\n",
    "                                                 bgsub=bgsub, fpsf=fpsf,\n",
    "                                                 blur=options.blur,\n",
    "                                                 exposure_=exposure_,\n",
    "                                                 visitid_=visitid_,\n",
    "                                                 vgroupid_=vgroupid_,\n",
    "                                                 psf=psf if refit_psf else None,\n",
    "                                                 options=options,\n",
    "                                                 suffix=f\"nsky{nsky}\")\n",
    "\n",
    "                zoomcut = slice(128, 256), slice(128, 256)\n",
    "\n",
    "                try:\n",
    "                    catalog_zoom_diagnostic(data, modsky, nullslice, stars)\n",
    "                    pl.suptitle(f\"Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_} nsky={nsky} weighted\")\n",
    "                    pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}_nsky{nsky}_weighted_catalog_diagnostics.png',\n",
    "                            bbox_inches='tight')\n",
    "\n",
    "                    catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                    pl.suptitle(f\"Catalog Diagnostics zoomed {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_} nsky={nsky} weighted\")\n",
    "                    pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}_nsky{nsky}_weighted_catalog_diagnostics_zoom.png',\n",
    "                            bbox_inches='tight')\n",
    "\n",
    "                    for name, zoomcut in zoomcut_list.items():\n",
    "                        catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                        pl.suptitle(f\"Crowdsource nsky={nsky} weighted Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_} zoom {name}\")\n",
    "                        pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{fpsf}{blur_}_nsky{nsky}_weighted_catalog_diagnostics_zoom{name.replace(\" \",\"_\")}.png',\n",
    "                                bbox_inches='tight')\n",
    "                except Exception as ex:\n",
    "                    print(f'FAILURE to produce catalog zoom diagnostics for module {module} and filter {filtername} for crowdsource nsky={nsky} refitpsf={refit_psf} blur={options.blur}: {ex}')\n",
    "                    exc_tb = sys.exc_info()[2]\n",
    "                    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                    print(f\"Exception {ex} was in {fname} line {exc_tb.tb_lineno}\")\n",
    "\n",
    "    if options.daophot:\n",
    "        t0 = time.time()\n",
    "        print(\"Starting basic PSF photometry\", flush=True)\n",
    "\n",
    "        if options.epsf:\n",
    "            print(\"Building EPSF\")\n",
    "            epsf_builder = EPSFBuilder(oversampling=3, maxiters=10,\n",
    "                                       smoothing_kernel='quadratic',\n",
    "                                       progress_bar=True)\n",
    "\n",
    "            epsfsel = ((finstars['peak'] > 200) &\n",
    "                       (finstars['roundness1'] > -0.25) &\n",
    "                       (finstars['roundness1'] < 0.25) &\n",
    "                       (finstars['roundness2'] > -0.25) &\n",
    "                       (finstars['roundness2'] < 0.25) &\n",
    "                       (finstars['sharpness'] > 0.4) &\n",
    "                       (finstars['sharpness'] < 0.8))\n",
    "\n",
    "            print(f\"Extracting {epsfsel.sum()} stars\")\n",
    "            # TODO: we might need to figure out how to tell extract_stars what's masked\n",
    "            stars = extract_stars(NDData(data=nan_replaced_data), finstars[epsfsel], size=25)\n",
    "\n",
    "            # reject stars with negative pixels\n",
    "            #stars = EPSFStars([x for x in stars if x.data.min() >= 0])\n",
    "            # apparently this failed - too restrictive?\n",
    "\n",
    "            for star in stars:\n",
    "                # background subtraction\n",
    "                star.data[:] -= np.nanpercentile(star.data, 5)\n",
    "\n",
    "            epsf, fitted_stars = epsf_builder(stars)\n",
    "\n",
    "            # trim edges\n",
    "            epsf._data = epsf.data[2:-2, 2:-2]\n",
    "\n",
    "            norm = simple_norm(epsf.data, 'log', percent=99.0)\n",
    "            pl.figure(1).clf()\n",
    "            pl.imshow(epsf.data, norm=norm, origin='lower', cmap='viridis')\n",
    "            pl.colorbar()\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}_daophot_epsf.png',\n",
    "                       bbox_inches='tight')\n",
    "            dao_psf_model = epsf\n",
    "\n",
    "            save_epsf(epsf,\n",
    "                      f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}_daophot_epsf.fits')\n",
    "\n",
    "\n",
    "        phot_basic = PSFPhotometry(finder=daofind_tuned,#finder_maker(),\n",
    "                             #grouper=grouper, # the grouper is needed to jointly fit stars and avoid faint stars jumping onto bright ones\n",
    "                             grouper=grouper if options.group else None,\n",
    "                             # localbkg_estimator=None, # must be none or it un-saturates pixels\n",
    "                             localbkg_estimator=LocalBackground(5, 15),\n",
    "                             psf_model=dao_psf_model,\n",
    "                             fitter=LevMarLSQFitter(),\n",
    "                             fit_shape=(5, 5),\n",
    "                             aperture_radius=2*fwhm_pix,\n",
    "                             progress_bar=True,\n",
    "                            )\n",
    "\n",
    "        print(\"About to do BASIC photometry....\")\n",
    "        result = phot_basic(nan_replaced_data, mask=mask)\n",
    "        # I want to use daofind params in the future\n",
    "        result['roundness1'] = finstars['roundness1']\n",
    "        result['roundness2'] = finstars['roundness2']\n",
    "        result['sharpness'] = finstars['sharpness']\n",
    "        print(f\"Done with BASIC photometry.  len(result)={len(result)} dt={time.time() - t0}\")\n",
    "\n",
    "        # remove negative-peak and zero-peak sources (they affect the residuals badly)\n",
    "        # we don't want to remove them now; we need to flag out objects that are too close to negative stars\n",
    "        #bad = result['flux_fit'] <= 0\n",
    "        #result = result[~bad]\n",
    "\n",
    "        result = save_photutils_results(result, ww, filename,\n",
    "                                        im1=im1, detector=detector,\n",
    "                                        basepath=basepath,\n",
    "                                        filtername=filtername, module=module,\n",
    "                                        desat=desat, bgsub=bgsub,\n",
    "                                        blur=options.blur,\n",
    "                                        exposure_=exposure_,\n",
    "                                        visitid_=visitid_,\n",
    "                                        vgroupid_=vgroupid_,\n",
    "                                        basic_or_iterative='basic',\n",
    "                                        options=options,\n",
    "                                        epsf_=epsf_,\n",
    "                                        group=group,\n",
    "                                        psf=None)\n",
    "\n",
    "        stars = result\n",
    "        stars['x'] = stars['x_fit']\n",
    "        stars['y'] = stars['y_fit']\n",
    "        print(\"Creating BASIC residual image, using 21x21 patches\")\n",
    "        modsky = phot_basic.make_model_image(data.shape, psf_shape=(21, 21), include_localbkg=False)\n",
    "        residual = data - modsky\n",
    "        print(\"Done creating BASIC residual image, using 21x21 patches\")\n",
    "        fits.PrimaryHDU(data=residual, header=im1[1].header).writeto(\n",
    "            f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_basic_residual.fits',\n",
    "            overwrite=True)\n",
    "        fits.PrimaryHDU(data=modsky, header=im1[1].header).writeto(\n",
    "            f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_basic_model.fits',\n",
    "            overwrite=True)\n",
    "        print(\"Saved BASIC residual image, now making diagnostics.\")\n",
    "        try:\n",
    "            catalog_zoom_diagnostic(data, modsky, nullslice, stars)\n",
    "            pl.suptitle(f\"daophot basic Catalog Diagnostics zoomed {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}\")\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_catalog_diagnostics_daophot_basic.png',\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "            catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "            pl.suptitle(f\"daophot basic Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}\")\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_catalog_diagnostics_zoom_daophot_basic.png',\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "            for name, zoomcut in zoomcut_list.items():\n",
    "                catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                pl.suptitle(f\"daophot basic Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group} zoom {name}\")\n",
    "                pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}__catalog_diagnostics_zoom_daophot_basic{name.replace(\" \",\"_\")}.png',\n",
    "                        bbox_inches='tight')\n",
    "        except Exception as ex:\n",
    "            print(f'FAILURE to produce catalog zoom diagnostics for module {module} and filter {filtername} BASIC photometry: {ex}')\n",
    "            exc_tb = sys.exc_info()[2]\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(f\"Exception {ex} was in {fname} line {exc_tb.tb_lineno}\")\n",
    "\n",
    "        print(f\"Done with diagnostics for BASIC photometry.  dt={time.time() - t0}\")\n",
    "        pl.close('all')\n",
    "\n",
    "    if options.daophot:\n",
    "        t0 = time.time()\n",
    "\n",
    "        print(\"Iterative PSF photometry\")\n",
    "        if options.epsf:\n",
    "            print(\"Building EPSF\")\n",
    "            epsf_builder = EPSFBuilder(oversampling=3, maxiters=10,\n",
    "                                       smoothing_kernel='quadratic',\n",
    "                                       progress_bar=True)\n",
    "\n",
    "            epsfsel = ((finstars['peak'] > 200) &\n",
    "                       (finstars['roundness1'] > -0.25) &\n",
    "                       (finstars['roundness1'] < 0.25) &\n",
    "                       (finstars['roundness2'] > -0.25) &\n",
    "                       (finstars['roundness2'] < 0.25) &\n",
    "                       (finstars['sharpness'] > 0.4) &\n",
    "                       (finstars['sharpness'] < 0.8))\n",
    "\n",
    "            print(f\"Extracting {epsfsel.sum()} stars\")\n",
    "            stars = extract_stars(NDData(data=nan_replaced_data), finstars[epsfsel], size=35)\n",
    "\n",
    "            # reject stars with negative pixels\n",
    "            #stars = EPSFStars([x for x in stars if x.data.min() >= 0])\n",
    "            # apparently this failed - too restrictive?\n",
    "\n",
    "            for star in stars:\n",
    "                # background subtraction\n",
    "                star.data[:] -= np.nanpercentile(star.data, 5)\n",
    "\n",
    "\n",
    "            epsf, fitted_stars = epsf_builder(stars)\n",
    "\n",
    "            # trim edges\n",
    "            epsf._data = epsf.data[2:-2, 2:-2]\n",
    "\n",
    "            norm = simple_norm(epsf.data, 'log', percent=99.0)\n",
    "            pl.figure(1).clf()\n",
    "            pl.imshow(epsf.data, norm=norm, origin='lower', cmap='viridis')\n",
    "            pl.colorbar()\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_epsf.png',\n",
    "                       bbox_inches='tight')\n",
    "            dao_psf_model = epsf\n",
    "\n",
    "        # iterative takes for-ev-er\n",
    "        phot_iter = IterativePSFPhotometry(finder=daofind_tuned,\n",
    "                                           localbkg_estimator=LocalBackground(5, 15),\n",
    "                                           grouper=grouper if options.group else None,\n",
    "                                           psf_model=dao_psf_model,\n",
    "                                           fitter=LevMarLSQFitter(),\n",
    "                                           maxiters=5,\n",
    "                                           fit_shape=(5, 5),\n",
    "                                           sub_shape=(15, 15),\n",
    "                                           aperture_radius=2*fwhm_pix,\n",
    "                                           progress_bar=True,\n",
    "                                          )\n",
    "\n",
    "        print(\"About to do ITERATIVE photometry....\")\n",
    "        result2 = phot_iter(nan_replaced_data, mask=mask)\n",
    "        print(f\"Done with ITERATIVE photometry. len(result2)={len(result2)}  dt={time.time() - t0}\")\n",
    "\n",
    "        # need to flag stars near negative stars, so we don't want to exclude them _yet_\n",
    "        #bad = result2['flux_fit'] <= 0\n",
    "        #result2 = result2[~bad]\n",
    "\n",
    "        result2 = save_photutils_results(result2, ww, filename,\n",
    "                                         im1=im1, detector=detector,\n",
    "                                         basepath=basepath,\n",
    "                                         filtername=filtername, module=module,\n",
    "                                         desat=desat, bgsub=bgsub,\n",
    "                                         blur=options.blur,\n",
    "                                         exposure_=exposure_,\n",
    "                                         visitid_=visitid_,\n",
    "                                         vgroupid_=vgroupid_,\n",
    "                                         basic_or_iterative='iterative',\n",
    "                                         options=options,\n",
    "                                         epsf_=epsf_,\n",
    "                                         group=group,\n",
    "                                         psf=None)\n",
    "\n",
    "        stars = result2\n",
    "        stars['x'] = stars['x_fit']\n",
    "        stars['y'] = stars['y_fit']\n",
    "\n",
    "        print(\"Creating iterative residual\")\n",
    "        modsky = phot_iter.make_model_image(data.shape, psf_shape=(21, 21), include_localbkg=False)\n",
    "        residual = data - modsky\n",
    "        print(\"finished iterative residual\")\n",
    "        fits.PrimaryHDU(data=residual, header=im1[1].header).writeto(\n",
    "            f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_iterative_residual.fits',\n",
    "            overwrite=True)\n",
    "        fits.PrimaryHDU(data=modsky, header=im1[1].header).writeto(\n",
    "            f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_daophot_iterative_model.fits',\n",
    "            overwrite=True)\n",
    "        print(\"Saved iterative residual\")\n",
    "        try:\n",
    "            catalog_zoom_diagnostic(data, modsky, nullslice, stars)\n",
    "            pl.suptitle(f\"daophot iterative Catalog Diagnostics zoomed {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}\")\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_catalog_diagnostics_daophot_iterative.png',\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "            catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "            pl.suptitle(f\"daophot iterative Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}\")\n",
    "            pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}_catalog_diagnostics_zoom_daophot_iterative.png',\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "            for name, zoomcut in zoomcut_list.items():\n",
    "                catalog_zoom_diagnostic(data, modsky, zoomcut, stars)\n",
    "                pl.suptitle(f\"daophot iterative Catalog Diagnostics {filtername} {module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group} zoom {name}\")\n",
    "                pl.savefig(f'{basepath}/{filtername}/pipeline/jw0{proposal_id}-o{field}_t001_nircam_{pupil}-{filtername.lower()}-{module}{visitid_}{vgroupid_}{exposure_}{desat}{bgsub}{epsf_}{blur_}{group}__catalog_diagnostics_zoom_daophot_iterative{name.replace(\" \",\"_\")}.png',\n",
    "                        bbox_inches='tight')\n",
    "        except Exception as ex:\n",
    "            print(f'FAILURE to produce catalog zoom diagnostics for module {module} and filter {filtername} for ITERATIVE daophot: {ex}')\n",
    "            exc_tb = sys.exc_info()[2]\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(f\"Exception {ex} was in {fname} line {exc_tb.tb_lineno}\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
